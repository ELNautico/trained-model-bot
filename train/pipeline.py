"""
train/pipeline.py
End-to-end pipeline with automatic *feature pruning*.

Flow
----
1. Download & cache OHLCV
2. Feature-engineer full set
3. If a pruned-feature file exists â†’ use it
4. Hyper-parameter tuning (walk-forward, via transformer_utils)
5. Sensitivity analysis â†’ drop weakest P% features
6. Retrain ensemble on pruned features
7. Forecast price & ÏƒÌ‚ and forward them to risk module
"""
from __future__ import annotations
from core.features import enrich_features, get_feature_columns

import hashlib
import json
import logging
from datetime import datetime, timedelta
from pathlib import Path
from zoneinfo import ZoneInfo

import joblib
import numpy as np
import pandas as pd
import toml
from sklearn.preprocessing import MinMaxScaler
from tenacity import retry, retry_if_exception_type, stop_after_attempt, wait_exponential
from twelvedata import TDClient

from core.features import enrich_features, get_feature_columns
from core.risk import generate_certificate
from core.sensitivity import compute_feature_sensitivity
from train.core import load_model, predict_price, train_and_save_model
from train.evaluate import backtest_strategy

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CONFIG
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_cfg_path = Path(__file__).resolve().parent.parent / "config.toml"
api_key = toml.load(_cfg_path)["twelvedata"]["api_key"]
td = TDClient(apikey=api_key)

CACHE_DIR = Path("cache")
CACHE_DIR.mkdir(exist_ok=True)

CACHE_TTL = timedelta(hours=24)
# Drop only truly absurd daily jumps (likely bad data). Adjusted prices should
# already account for splits/dividends; keep this generous to avoid losing days.
MAX_RETURN_THRESHOLD = 0.50
MAX_STALE_DAYS = 2
PRUNE_PCT = 30                      # bottom X % features to drop
ADJUSTMENT_MODE = "all"             # "all" or "split" or None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  Helpers Â­â€“ I/O
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _pruned_path(ticker: str) -> Path:
    """models/TKR/LATEST_pruned_features.json"""
    return Path("models") / ticker / "LATEST_pruned_features.json"


def load_pruned_features(ticker: str) -> list[str] | None:
    path = _pruned_path(ticker)
    if path.exists():
        try:
            return json.loads(path.read_text())
        except Exception:
            logging.warning("âš ï¸  Failed to read pruned-feature list for %s", ticker)
    return None


def save_pruned_features(ticker: str, pruned: list[str], version_dir: Path):
    latest = _pruned_path(ticker)
    try:
        latest.write_text(json.dumps(pruned, indent=2))
        (version_dir / "pruned_features.json").write_text(json.dumps(pruned, indent=2))
    except Exception as e:
        logging.warning("âš ï¸  Could not persist pruned features: %s", e)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  Raw data download + clean
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Keywords that indicate a TwelveData rate-limit / quota exhaustion response.
_RATELIMIT_KEYWORDS = (
    "credit", "rate limit", "rate_limit", "quota",
    "too many", "429", "limit reached", "exceeded", "throttl",
)


def _is_ratelimit_error(exc: Exception) -> bool:
    """Return True if *exc* looks like a TwelveData rate-limit or quota error."""
    msg = str(exc).lower()
    return any(kw in msg for kw in _RATELIMIT_KEYWORDS)


def _download_raw_ohlcv_yfinance(ticker: str) -> pd.DataFrame:
    """
    Last-resort OHLCV download via yfinance.

    Used when TwelveData (SDK + HTTP) is rate-limited or unavailable.
    auto_adjust=True mirrors TwelveData adjustment="all" (splits + dividends).
    """
    import yfinance as yf

    logging.info("%s: downloading from yfinance (fallback)", ticker)
    df = yf.download(
        ticker,
        period="max",
        interval="1d",
        auto_adjust=True,
        progress=False,
    )

    if df.empty:
        raise ValueError(f"yfinance returned empty data for {ticker}")

    # yfinance â‰¥ 0.2 returns flat columns for a single ticker, but guard anyway.
    if hasattr(df.columns, "levels"):
        df.columns = df.columns.get_level_values(0)

    # Keep only standard OHLCV columns (yfinance may include Dividends etc.)
    cols = [c for c in ("Open", "High", "Low", "Close", "Volume") if c in df.columns]
    df = df[cols].copy().astype(float)

    # Ensure UTC-aware DatetimeIndex (yfinance daily bars are often tz-naive).
    if df.index.tz is None:
        df.index = pd.to_datetime(df.index).tz_localize("UTC")
    else:
        df.index = df.index.tz_convert("UTC")
    df.index.name = "datetime"

    logging.info("%s: yfinance returned %d rows", ticker, len(df))
    return df.sort_index()


@retry(
    retry=retry_if_exception_type(Exception),
    wait=wait_exponential(multiplier=1, min=1, max=10),
    stop=stop_after_attempt(3),
)
def _download_raw_ohlcv(ticker: str) -> pd.DataFrame:
    """
    Download OHLCV with a three-level fallback chain:

      1. TwelveData SDK  (preferred â€“ supports adjustment parameter)
      2. TwelveData HTTP (if SDK raises TypeError for the adjustment kwarg)
      3. yfinance        (if TwelveData is rate-limited / quota exhausted)

    Rate-limit errors short-circuit to the next level immediately; other errors
    are propagated so that tenacity can retry the whole chain.
    """
    # â”€â”€ 1. TwelveData SDK â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    try:
        df = td.time_series(
            symbol=ticker,
            interval="1day",
            outputsize=5000,
            timezone="UTC",
            adjustment=ADJUSTMENT_MODE,
        ).as_pandas()
        return df.sort_index()
    except TypeError:
        # Older SDK build: 'adjustment' kwarg not supported â†’ fall to HTTP.
        logging.warning(
            "âš ï¸  TDClient.time_series doesn't support 'adjustment'; "
            "trying HTTP fallback (adjustment=%s)", ADJUSTMENT_MODE
        )
    except Exception as exc:
        if _is_ratelimit_error(exc):
            logging.warning(
                "âš ï¸  TwelveData SDK rate-limited for %s (%s); trying HTTPâ€¦",
                ticker, exc,
            )
        else:
            raise  # transient network error â†’ let tenacity retry

    # â”€â”€ 2. TwelveData HTTP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    try:
        return _download_raw_ohlcv_http(ticker)
    except Exception as exc:
        if _is_ratelimit_error(exc):
            logging.warning(
                "âš ï¸  TwelveData HTTP rate-limited for %s (%s); "
                "falling back to yfinanceâ€¦", ticker, exc,
            )
        else:
            raise  # transient error â†’ let tenacity retry

    # â”€â”€ 3. yfinance fallback â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    return _download_raw_ohlcv_yfinance(ticker)


def cached_download(ticker: str) -> pd.DataFrame:
    cache_key = f"{ticker}|adj={ADJUSTMENT_MODE}"
    key = hashlib.md5(cache_key.encode()).hexdigest()
    path = CACHE_DIR / f"{key}.pkl"

    if path.exists() and datetime.utcnow() - datetime.utcfromtimestamp(path.stat().st_mtime) < CACHE_TTL:
        df = joblib.load(path)
    else:
        df = _download_raw_ohlcv(ticker)
        joblib.dump(df, path)

    # Soft filter for extreme outliers; log how many rows would be dropped.
    df["ret"] = df["Close"].pct_change()
    bad_mask = df["ret"].abs() > MAX_RETURN_THRESHOLD
    bad_count = int(bad_mask.sum())
    if bad_count:
        logging.warning("âš ï¸  %s: dropping %d rows with > %.0f%% daily change",
                        ticker, bad_count, MAX_RETURN_THRESHOLD * 100)
        df = df.loc[~bad_mask]
    df = df.drop(columns="ret")

    # Do not drop rows after long gaps; retain data and let models handle.
    return df

def _download_raw_ohlcv_http(ticker: str) -> pd.DataFrame:
    import requests

    url = "https://api.twelvedata.com/time_series"
    params = {
        "symbol": ticker,
        "interval": "1day",
        "outputsize": 5000,
        "timezone": "UTC",
        "adjustment": ADJUSTMENT_MODE,
        "apikey": api_key,
    }
    r = requests.get(url, params=params, timeout=30)
    r.raise_for_status()
    data = r.json()

    if isinstance(data, dict) and data.get("status") == "error":
        raise ValueError(f"TwelveData error: {data.get('message')}")

    values = data.get("values", [])
    if not values:
        raise ValueError(f"Empty OHLCV for {ticker} (HTTP fallback).")

    df = pd.DataFrame(values)
    # TwelveData commonly returns strings; normalize
    df["datetime"] = pd.to_datetime(df["datetime"], utc=True)
    df = df.set_index("datetime").sort_index()
    df = df.rename(columns={
        "open": "Open", "high": "High", "low": "Low", "close": "Close", "volume": "Volume"
    })
    df = df[["Open","High","Low","Close","Volume"]].astype(float)
    return df

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  Sequence builder (d1 & d5 only)         << UPDATED
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_sequences(
    df: pd.DataFrame,
    feats: list[str],
    *,
    window_size: int,
    test_ratio: float = 0.2,
):
    close = df["Close"].values

    df = df[feats].ffill().bfill()

    split = int(len(df) * (1 - test_ratio))
    train_df, test_df = df.iloc[:split], df.iloc[split:]

    scaler = MinMaxScaler()
    train_sc = scaler.fit_transform(train_df)
    test_sc  = scaler.transform(test_df)
    sc_all   = np.vstack([train_sc, test_sc])

    X_tr, X_te = [], []
    y1_tr, y5_tr = [], []
    y1_te, y5_te = [], []

    last_idx = len(df) - 5

    def _append(X, y1, y5, i):
        window = sc_all[i - window_size : i]
        # Only append if window is correct shape
        if window.shape == (window_size, len(feats)):
            X.append(window)
            y1.append(np.log(close[i] / close[i - 1]))        # 1-day return
            y5.append(np.log(close[i + 4] / close[i - 1]))    # 5-day return

    for i in range(window_size, split - 5):
        _append(X_tr, y1_tr, y5_tr, i)
    for i in range(split, last_idx):
        _append(X_te, y1_te, y5_te, i)

    y_train = {"d1": np.array(y1_tr), "d5": np.array(y5_tr)}
    y_test  = {"d1": np.array(y1_te), "d5": np.array(y5_te)}

    return (
        np.array(X_tr),
        y_train,
        np.array(X_te),
        y_test,
        scaler,
    )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  Main orchestrator
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def train_predict_for_ticker(
    ticker: str,
    use_ensemble: bool = True,      # â† works both positionally *and* as keyword
    account_balance: float = 100_000.0,
    risk_per_trade: float = 0.01,
):
    logging.info("ðŸš€ %s â€“ pipeline start", ticker)

    df = cached_download(ticker)
    # Pass ticker to DataFrame attrs for sentiment feature
    df.attrs['ticker'] = ticker
    df = enrich_features(df)

    # â”€â”€ feature subset (may be pruned)
    # Find latest model version directory
    model_base = Path("models") / ticker
    import re
    ts_pattern = re.compile(r"^\d{8}_\d{6}$")
    versions = [d for d in model_base.iterdir() if d.is_dir() and ts_pattern.match(d.name)]
    versions = sorted(versions, reverse=True)
    if not versions:
        logging.error(f"âŒ No versioned subdirectory found for {ticker} in models/")
        raise FileNotFoundError(f"No versioned subdirectory found for {ticker} in models/")
    latest_version_dir = versions[0]
    # Load actual feature list from metadata.json in latest model dir
    metadata_path = latest_version_dir / "metadata.json"
    if metadata_path.exists():
        with open(metadata_path, "r", encoding="utf-8") as f:
            metadata = json.load(f)
        # If features are saved as a list of names, use them; if shape, fallback to get_feature_columns()
        feats_in_use = metadata.get("feature_names") if "feature_names" in metadata else get_feature_columns()
    else:
        feats_in_use = get_feature_columns()
    win = 60 if len(df) >= 600 else 30
    X_tr, y_tr, X_te, y_te, scaler = build_sequences(df, feats_in_use, window_size=win)

    # â”€â”€ model load only
    model_base = Path("models") / ticker
    if not model_base.exists():
        logging.error(f"âŒ No model directory found for {ticker} in models/")
        raise FileNotFoundError(f"No model directory found for {ticker} in models/")
    # Only use timestamped subdirectories (ignore files like LATEST_pruned_features.json)
    import re
    ts_pattern = re.compile(r"^\d{8}_\d{6}$")
    versions = [d for d in model_base.iterdir() if d.is_dir() and ts_pattern.match(d.name)]
    versions = sorted(versions, reverse=True)
    if not versions:
        logging.error(f"âŒ No versioned subdirectory found for {ticker} in models/")
        raise FileNotFoundError(f"No versioned subdirectory found for {ticker} in models/")
    expected_model_path = versions[0] / "model.h5"
    logging.info(f"ðŸ”Ž Checking for model at: {expected_model_path}")
    if expected_model_path.exists():
        logging.info(f"âœ… Found model for {ticker} at {expected_model_path}")
        try:
            latest_version = versions[0].name
            model = load_model(ticker, version_timestamp=latest_version)
            logging.info(f"ðŸ“¦ model loaded from {expected_model_path}")
        except FileNotFoundError as e:
            logging.error(f"âŒ Model loading failed: {e}")
            raise
    else:
        logging.error(f"âŒ Model file missing at {expected_model_path}")
        raise FileNotFoundError(f"Model file missing at {expected_model_path}")

    # â”€â”€ forecast
    current_px = float(df["Close"].iloc[-1])
    last_seq = X_te[-1:][...]
    pred_px, pred_vol = predict_price(model, last_seq, current_px)
    final_px = 0.9 * pred_px + 0.1 * current_px if use_ensemble else pred_px

    hist_vol = float(df["Close"].pct_change().std())
    vol_used = float(np.clip(pred_vol, 0.5 * hist_vol, 2.0 * hist_vol))

    pct_change = (final_px / current_px - 1) * 100
    confidence = round(min(abs(pct_change) * 10, 100), 1)
    direction = "Buy" if pct_change > 0 else "Sell" if pct_change < 0 else "Hold"
    acc, cum_ret = backtest_strategy(model, X_te, df, win)

    ts_local = df.index[-1].tz_convert(ZoneInfo("Europe/Vienna"))
    cert = generate_certificate(current_px, final_px, vol_used)
    sens_out = compute_feature_sensitivity(model, last_seq, feats_in_use)

    return (
        {
            "Ticker": ticker,
            "Timestamp": ts_local.strftime("%Y-%m-%d %H:%M %Z"),
            "Current Price": round(current_px, 2),
            "Predicted Price": round(final_px, 2),
            "Predicted % Change": round(pct_change, 2),
            "Confidence": confidence,
            "Volatility": round(vol_used, 4),
            "Directional Accuracy": round(acc, 4),
            "Cumulative Return": round(cum_ret, 4),
            "Recommended Certificate": cert,
            "Feature Sensitivity": sens_out,
            "Feature Count": len(feats_in_use),
        },
        df,
    )

# --------------------------------------------------------------------------
# Back-compat aliases (used by jobs.py  and Telegram-Bot)
# --------------------------------------------------------------------------

def prepare_data_and_split(
    df: pd.DataFrame,
    *,
    window_size: int = 60,
    test_ratio: float = 0.2,
):
    """
    Legacy-Wrapper fÃ¼r alten Code (retrain, Bot).
    Rechnet Features, ruft build_sequences() und liefert
    dieselbe 6-teilige Tuple-Struktur wie frÃ¼her zurÃ¼ck.
    """
    df = enrich_features(df)

    # Ensure all required feature columns exist, fill missing with NaN
    for col in get_feature_columns():
        if col not in df.columns:
            df[col] = np.nan
    feats = [f for f in get_feature_columns() if f in df.columns]
    X_tr, y_tr, X_te, y_te, scaler = build_sequences(
        df,
        feats,
        window_size=window_size,
        test_ratio=test_ratio,
    )
    train_size = len(X_tr)
    return X_tr, y_tr, X_te, y_te, scaler, train_size


# keep old alias
download_data = cached_download
